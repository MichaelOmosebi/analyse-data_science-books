{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a3bc28-a53f-4f95-b1ee-3fe93a478c62",
   "metadata": {},
   "source": [
    "# <font color=blue> SCRAPE GOODREADS BOOKS AND REVIEWS </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26b57c-0151-4031-891e-4f01dbce916f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.0 INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51246e57-c5ea-4eb6-8bc3-90c6093c8712",
   "metadata": {},
   "source": [
    "##### This notebook contains code which serves as ```Part I``` of an E-2-E project on the sentiments analysis of Data Science books reviews.\n",
    "\n",
    "##### The motivation for this project came from my need to answer the question of: \"Which Data Science Book(s) to read?\" . While on that thought I came across similar projects which also inspired me to boraden the scope of the project. On this project I will br doing the following:\n",
    "\n",
    "* Scrape basic data on 'Data Science' books from Goodreads.\n",
    "* Do Some EDA around cost per book, and comparison of other book features with cost as a ground property.\n",
    "* Explore the variety topics available on the subject, and its related subjects.\n",
    "* Scrape reviews and stars ✨ , as the second scraping run\n",
    "* Attempt to compare the result of sentiments analaysis from different models on the true ratings, per review comment and the overall book rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e5640-6c9b-4aa2-9962-94d25a0a6204",
   "metadata": {},
   "source": [
    "#### 1.1 IMPORT PACKAGES & DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce6e62d-ff78-413f-8a9a-9d1de69190c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import image module\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310b2058-6420-497f-9c38-94f2a304daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define headers property needed by the requests library for scraping\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e0ecd-17a3-46c7-b0b0-d5a8c9d1e5db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.0 STEP-WISE DATA COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe4ebd-5fbb-4180-a8b4-584d8741eea8",
   "metadata": {},
   "source": [
    "#### 2.1 Collect Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceac7a89-1292-4b79-876a-d6c05f558d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.goodreads.com/search?page=1&qid=lolPVmy4Sz&query=data+science&tab=books&utf8=%E2%9C%93'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decalre variables describing the URL/page to be scraped\n",
    "\n",
    "search_query = 'data science'.replace(' ', '+')\n",
    "page_num=1\n",
    "base_url = f'https://www.goodreads.com/search?page={page_num}&qid=lolPVmy4Sz&query={search_query}&tab=books&utf8=%E2%9C%93'#.format(search_query)\n",
    "\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645b42cf-1527-4ed8-b3a5-5debfc25b810",
   "metadata": {},
   "source": [
    "***\n",
    "THE BOOK SHELF\n",
    "***\n",
    "\n",
    "* All books are housed within the table object, as seen in the image below.\n",
    "* Each book object (with their properties) are stored in ```tr``` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd6fcb-5e61-4c2d-ae09-77ed1b6fff84",
   "metadata": {},
   "source": [
    "<img src=\"images\\books_shelve1.png\" width=\"900\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f415b345-780f-46fc-ab9b-0bed8e6eeb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n \\n\\n\\nData Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\\n \\nby\\n\\n\\nFoster Provost, \\n\\n\\nTom Fawcett\\n\\n\\n\\n\\n\\n 4.13 avg rating — 2,337 ratings\\n              —\\n                published\\n               2013\\n              —\\n              23 editions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant to Read\\nsaving…\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWant to Read\\n\\n\\n\\n\\nCurrently Reading\\n\\n\\n\\n\\nRead\\n\\n\\n\\n\\n\\n\\n\\n\\nError rating book. Refresh and try again.\\n\\nRate this book\\nClear rating\\n1 of 5 stars2 of 5 stars3 of 5 stars4 of 5 stars5 of 5 stars\\n\\n\\n\\n\\nGet a copy\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect content of desired page\n",
    "firstpage_content = requests.get(base_url, headers=headers)\n",
    "\n",
    "#prettify the html content\n",
    "soup = BeautifulSoup(firstpage_content.text, 'lxml')\n",
    "\n",
    "#book shelve\n",
    "books_shelve = soup.find('table').find_all('tr')\n",
    "\n",
    "#print properties of the first book on the shelve\n",
    "book = books_shelve[0]\n",
    "\n",
    "book.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9cc11a-2a6f-432c-970d-472151a38ad3",
   "metadata": {},
   "source": [
    "***\n",
    "From the spilled content above we see that the each book object on the book shelve provides the following basic info about the book:\n",
    "1. Book title\n",
    "2. Book Author & Co-author\n",
    "3. Publication information\n",
    "4. Overall book rating\n",
    "\n",
    "The next line we will show how to neatly collect these information from the book objects.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27d6ae-97db-4a5a-92bb-3de52febf1bb",
   "metadata": {},
   "source": [
    "<img src=\"images\\basic_book_details.png\" width=\"900\" height=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f89983-1c71-40e5-97b9-2749f902f12a",
   "metadata": {},
   "source": [
    "#### 2.2 Extract the Basci Info from ```Soup``` of Book Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0efec64d-1bbf-4004-878e-47317a70b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking \n",
      " ['Foster Provost', 'Tom Fawcett'] \n",
      "  4.13 \n",
      " 23 editions\n"
     ]
    }
   ],
   "source": [
    "#book title and author\n",
    "book_title = book.a['title']\n",
    "\n",
    "author = book.find_all('span')[2].text\n",
    "\n",
    "#collects all authors and inserts in a list\n",
    "authors = book.find_all('div', {'class': 'authorName__container'})#.text\n",
    "author_names = [a.text.strip(', \\n') for a in authors]\n",
    "\n",
    "#rating info\n",
    "rating = book.find('span', {'class': 'minirating'}).text[0:5]\n",
    "rating_count = book.find('span', {'class': 'minirating'}).text.split(' — ')[1]\n",
    "\n",
    "#publication info\n",
    "editions = book.find('a', {'class': 'greyText'}).text\n",
    "\n",
    "#view some of the extracted details\n",
    "print(book_title, '\\n', author_names, '\\n', rating, '\\n', editions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1746939-57db-40a2-91eb-eb83197099a6",
   "metadata": {},
   "source": [
    "#### 2.3 Proceed to Getting In-Depth Book Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71cf5d-ce31-4cc4-8b52-8c7155349f85",
   "metadata": {},
   "source": [
    "##### 2.3 Extract ```href``` Property and Get the Details Page URL per Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1af48c-4e5a-46d5-98c7-26a5bb50cdaa",
   "metadata": {},
   "source": [
    "***\n",
    "An important property that accompanies the basic information but not shown as part of the content viewed above is the ```href``` property.\n",
    "\n",
    "The ```href``` property is a link extension that allows us to view in-depth details for each book, namely;\n",
    "\n",
    "1. Book Price (not all prices are available; some refer to Amazon)\n",
    "2. Foreward (if available)\n",
    "3. Book genre\n",
    "4. Reviews & stars for each review et.c\n",
    "\n",
    "In the next lines the ```href``` property in combination with the ```home-page url``` will be used to extracted deeper book details as outlined above.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7695875-6fde-4f15-b963-df6ce2abff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/book/show/17912916-data-science-for-business?from_search=true&from_srp=true&qid=lolPVmy4Sz&rank=1\n"
     ]
    }
   ],
   "source": [
    "#get href property\n",
    "href = book.a['href']\n",
    "print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3738abac-4aac-4b34-8361-81fccf739007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.goodreads.com/book/show/17912916-data-science-for-business?from_search=true&from_srp=true&qid=lolPVmy4Sz&rank=1\n"
     ]
    }
   ],
   "source": [
    "#combine with the home-page url to get the complete book address\n",
    "book_url='https://www.goodreads.com' + href\n",
    "print(book_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a063ebe-b93a-4ce3-9fc8-2d1645025768",
   "metadata": {},
   "source": [
    "##### 2.4 Preview the ```html Soup``` for the Main Book Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea920b3-606e-43aa-a8be-c4d7698c8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch the html content for the book's extra details\n",
    "bookdetails = requests.get(book_url, headers=headers)\n",
    "\n",
    "#prettify the html content\n",
    "soup1 = BeautifulSoup(bookdetails.text, 'lxml')\n",
    "\n",
    "#soup1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654c2f5-f35d-4363-aa82-c50ca9b03200",
   "metadata": {},
   "source": [
    "###### The cookdetails soup above contains all the 8 features outlined above, including the first 30 reviews. Being a large soup, the  reult will be sliced just so we have a brief view\n",
    "\n",
    "***\n",
    "As will be seen in a slice of the text/string output, we have more data (price, foreward, reviews, ratings distribution etc.)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caab53d6-b6ce-4ff1-832f-f099a2f13ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking by Foster Provost | GoodreadsHomeMy BooksBrowse ▾RecommendationsChoice AwardsGiveawaysNew ReleasesListsExploreNews & InterviewsLoading...Community ▾GroupsQuotesAsk the AuthorPeopleSign inJoinJump to ratings and reviewsWant to readKindle $25.49Rate this bookData Science for Business: What You Need to Know about Data Mining and Data-Analytic ThinkingFoster Provost, Tom Fawcett4.132,337\\xa0ratings152\\xa0reviewsWant to readKindle $25.49Rate this bookWritten by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the \"data-analytic thinking\" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You’ll not only learn how to improve communication between business stakeholders and dat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup1.text[:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627f8dd-2b0f-46f5-a090-37f1a00e943f",
   "metadata": {},
   "source": [
    "#### Important Notice⚠️\n",
    "***\n",
    "This page does not always make data avaialble at each call/request.\n",
    "\n",
    "For this reason book properties that requirie indexing, slicing operations would throw an error when the page fails to respond with data.\n",
    "\n",
    "To deal with this error however, the codes will be wrapped in _```try-except```_ block, especially during iterations to avoid code breakage.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a118c8-a418-469f-bef9-5299e680d223",
   "metadata": {},
   "source": [
    "#### 2.4 Collect All Basic Book Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0f6452-6997-411f-aaef-865a25b8ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate try-except blocks will be used for different properties as data response from each call is random for different properties\n",
    "#Foreward\n",
    "try:\n",
    "    price = soup1.find_all('div' , {'class':'BookActions__button'})[-1].text.split(' ')[1]\n",
    "except IndexError:\n",
    "    price = 'Buys on Amazon'\n",
    "\n",
    "try:\n",
    "    foreward = soup1.find('span' , {'class': 'Formatted', }).text\n",
    "except AttributeError:\n",
    "    foreward = 'Not Found'\n",
    "\n",
    "#Book genres\n",
    "try:\n",
    "    genres = soup1.find('ul' , {'class': 'CollapsableList'}).text\n",
    "except AttributeError:\n",
    "    genres = 'Not Found'\n",
    "\n",
    "try:\n",
    "    num_pages=soup1.find('p' , {'data-testid':'pagesFormat'}).text.split(', ')[0]\n",
    "except AttributeError:\n",
    "    num_pages = 'Not Available'\n",
    "\n",
    "try:\n",
    "    publication_info_firstedition = soup1.find('p' , {'data-testid':'publicationInfo'}).text #may not always be available. use tr,except block\n",
    "except AttributeError:\n",
    "    publication_info_firstedition = 'Not Available'\n",
    "\n",
    "try:\n",
    "    publication_info_firstedition1 = [i.text for i in soup1.find('div' , {'class':'FeaturedDetails'})]\n",
    "except (AttributeError, TypeError):\n",
    "    publication_info_firstedition1 = 'Not Available'\n",
    "\n",
    "first30_reviews=soup1.find_all('div' , {'class':'TruncatedContent__text'})\n",
    "reviews = [(rev.text, \"\\n\") for rev in first30_reviews]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6295d0e8-ff87-483a-b669-0896a96bcb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking\n",
      "\n",
      "\n",
      "Published ['413 pages, Paperback', 'First published January 1, 2013']\n",
      "\n",
      "\n",
      "Written by ['Foster Provost', 'Tom Fawcett'] with a rating of  4.13 from 2,337 ratings\n",
      "\n",
      "\n",
      "Costs $25.49\n"
     ]
    }
   ],
   "source": [
    "#view some of the extracted details\n",
    "print(book_title)\n",
    "print('\\n')\n",
    "print(f'Published {publication_info_firstedition1}')\n",
    "print('\\n')\n",
    "print(f'Written by {author_names} with a rating of {rating} from {rating_count}')\n",
    "print('\\n')\n",
    "print(f'Costs {price}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653878-9731-4f07-bbc1-dcc3f0218d79",
   "metadata": {},
   "source": [
    "### 3.0 PUTTING IT ALL TOGETHER AND SCRAPE FOR ALL BOOKS ON ALL SHELVES (PAGE)\n",
    "\n",
    "***\n",
    "The next code line shows how to extract overview data and main book data for each book on all existing shelve/web-page as at the time of this scrape. \n",
    "\n",
    "As at the time of this scrape there exists 101 pages of 'data science' related books on Goodreads website.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af97b850-135f-440f-875e-5db0f2de11e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [07:51<00:00, 235.96s/it]\n"
     ]
    }
   ],
   "source": [
    "#Iterate throught the 101 webpages of Goodreads 'data science' books\n",
    "items = []\n",
    "search_query = 'data science'.replace(' ', '+')\n",
    "\n",
    "\n",
    "for i in tqdm(range(1, 101)):\n",
    "    start = time.time()\n",
    "    \n",
    "    #create a URL for each page with the search query\n",
    "    page_url = f'https://www.goodreads.com/search?page={i}&qid=lolPVmy4Sz&query={search_query}&tab=books&utf8=%E2%9C%93'\n",
    "\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    sleep(5)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    #each page renders all books in a 'table' containier where all details are captured in the 'tr' tag\n",
    "    #wrap the search through the soup in a try-except block to capture exceptions and keep loop running\n",
    "    try:\n",
    "        results = soup.find('table').find_all('tr')\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "    #iterate through each book in the 'tr' tag containing all books to fetch overview information\n",
    "    for result in results:\n",
    "            \n",
    "        href = result.a['href']\n",
    "        title = result.a['title']\n",
    "        title1 = result.find_all('span')[0].text\n",
    "        author = result.find_all('span')[2].text\n",
    "        authors = result.find_all('div', {'class': 'authorName__container'})#.text\n",
    "        author_names = [a.text.strip(', \\n') for a in authors]\n",
    "        rating = result.find('span', {'class': 'minirating'}).text[0:5]\n",
    "        rating_count = result.find('span', {'class': 'minirating'}).text.split(' — ')[1]\n",
    "        editions = result.find('a', {'class': 'greyText'}).text\n",
    "        \n",
    "        #load the html for each book to fetch drill down info, that is not available on the overview\n",
    "        product_url='https://www.goodreads.com' + href\n",
    "        sleep(2)\n",
    "        response1 = requests.get(product_url, headers=headers)\n",
    "        sleep(5)\n",
    "        soup1 = BeautifulSoup(response1.text, 'lxml')\n",
    "        \n",
    "        #Foreward\n",
    "        try:\n",
    "            foreward = soup1.find('span' , {'class': 'Formatted', }).text\n",
    "        except AttributeError:\n",
    "            foreward = 'Not Found'\n",
    "            \n",
    "        try:\n",
    "            foreward1 = soup1.find_all('div' , {'class':'TruncatedContent__text'})[0].text\n",
    "        except IndexError:\n",
    "            foreward1 = 'Not Found'\n",
    "        \n",
    "        #Book genres\n",
    "        try:\n",
    "            genres = soup1.find('ul' , {'class': 'CollapsableList'}).text\n",
    "        except AttributeError:\n",
    "            genres = 'Not Found'\n",
    "        \n",
    "        try:\n",
    "            num_pages=soup1.find('p' , {'data-testid':'pagesFormat'}).text.split(', ')[0]\n",
    "        except AttributeError:\n",
    "            num_pages = 'Not Available'\n",
    "            \n",
    "        try:\n",
    "            paperback = soup1.find('p' , {'data-testid':'pagesFormat'}).text\n",
    "        except (AttributeError,IndexError):\n",
    "            paperback = 'Not Available'\n",
    "            \n",
    "        #paperback=soup1.find('p' , {'data-testid':'pagesFormat'}).text.split(', ')[1]\n",
    "        \n",
    "        try:\n",
    "            publication_info_firstedition = soup1.find('p' , {'data-testid':'publicationInfo'}).text #may not always be available. use tr,except block\n",
    "        except AttributeError:\n",
    "            publication_info_firstedition = 'Not Available'\n",
    "            \n",
    "        try:\n",
    "            publication_info_firstedition1 = [i.text for i in soup1.find('div' , {'class':'FeaturedDetails'})]\n",
    "        except (AttributeError, TypeError):\n",
    "            publication_info_firstedition1 = 'Not Available'\n",
    "            \n",
    "        first30_reviews=soup1.find_all('div' , {'class':'TruncatedContent__text'})\n",
    "        reviews = [(rev.text, \"\\n\") for rev in first30_reviews]\n",
    "        #soup1.find('div' , {'class':'TruncatedContent__text', 'class':'TruncatedContent__text--small'})\n",
    "        \n",
    "        try:\n",
    "            price = soup1.find_all('div' , {'class':'BookActions__button'})[-1].text.split(' ')[1]\n",
    "        except IndexError:\n",
    "            price = 'Buys on Amazon'\n",
    "        \n",
    "        try:\n",
    "            price1 = [i.text for i in soup1.find_all('span' , {'class':'Button__labelItem'}) if '$' in i.text]\n",
    "        except AttributeError:\n",
    "            price1 = 'Not Found'\n",
    "            \n",
    "        try:\n",
    "            rating1 = soup1.find('div' , {'class':'RatingStatistics__rating'}).text\n",
    "        except AttributeError:\n",
    "            rating1 = 'Not Found'\n",
    "            \n",
    "        try:\n",
    "            rating_count1 = soup1.find('span' , {'data-testid':'ratingsCount'}).text\n",
    "        except AttributeError:\n",
    "            rating_count1 = 'Not Found'\n",
    "        \n",
    "        items.append([title, title1, author, author_names, genres, num_pages, paperback, rating, rating1, rating_count, rating_count1, \n",
    "                       editions, publication_info_firstedition, publication_info_firstedition1, foreward, foreward1, price, price1,\n",
    "                       reviews, product_url])\n",
    "    #duration = time.time() - start\n",
    "    #print(f'Processed Page {i}... in {duration}')\n",
    "            \n",
    "    sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ec4f19-f245-4bf2-a0f0-1e53f9909563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>title1</th>\n",
       "      <th>author</th>\n",
       "      <th>author_names</th>\n",
       "      <th>genres</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>paperback</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating1</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>rating_count1</th>\n",
       "      <th>editions</th>\n",
       "      <th>publication_info_firstedition</th>\n",
       "      <th>publication_info_firstedition1</th>\n",
       "      <th>foreward</th>\n",
       "      <th>foreward1</th>\n",
       "      <th>price</th>\n",
       "      <th>price1</th>\n",
       "      <th>reviews</th>\n",
       "      <th>product_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science for Business: What You Need to Kn...</td>\n",
       "      <td>Data Science for Business: What You Need to Kn...</td>\n",
       "      <td>\\n\\nFoster Provost, \\n\\n\\nTom Fawcett\\n\\n</td>\n",
       "      <td>[Foster Provost, Tom Fawcett]</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>4.13</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>2,337 ratings</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>23 editions</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Buys on Amazon</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.goodreads.com/book/show/17912916-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Smart: Using Data Science to Transform In...</td>\n",
       "      <td>Data Smart: Using Data Science to Transform In...</td>\n",
       "      <td>\\n\\nJohn W. Foreman\\n\\n</td>\n",
       "      <td>[John W. Foreman]</td>\n",
       "      <td>GenresBusinessNonfictionTechnologyProgrammingS...</td>\n",
       "      <td>409 pages</td>\n",
       "      <td>409 pages, Paperback</td>\n",
       "      <td>4.11</td>\n",
       "      <td>4.11</td>\n",
       "      <td>988 ratings</td>\n",
       "      <td>988 ratings</td>\n",
       "      <td>5 editions</td>\n",
       "      <td>First published October 31, 2013</td>\n",
       "      <td>[409 pages, Paperback, First published October...</td>\n",
       "      <td>Data Science gets thrown around in the press l...</td>\n",
       "      <td>Data Science gets thrown around in the press l...</td>\n",
       "      <td>$27.00</td>\n",
       "      <td>[Kindle $27.00, Kindle $27.00]</td>\n",
       "      <td>[(Data Science gets thrown around in the press...</td>\n",
       "      <td>https://www.goodreads.com/book/show/17682206-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science from Scratch: First Principles wi...</td>\n",
       "      <td>Data Science from Scratch: First Principles wi...</td>\n",
       "      <td>\\n\\nJoel Grus\\n\\n</td>\n",
       "      <td>[Joel Grus]</td>\n",
       "      <td>GenresProgrammingComputer ScienceTechnologyNon...</td>\n",
       "      <td>330 pages</td>\n",
       "      <td>330 pages, Kindle Edition</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.91</td>\n",
       "      <td>1,018 ratings</td>\n",
       "      <td>1,018 ratings</td>\n",
       "      <td>25 editions</td>\n",
       "      <td>First published April 14, 2015</td>\n",
       "      <td>[330 pages, Kindle Edition, First published Ap...</td>\n",
       "      <td>\\nData science libraries, frameworks, modules,...</td>\n",
       "      <td>\\nData science libraries, frameworks, modules,...</td>\n",
       "      <td>on</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(\\nData science libraries, frameworks, module...</td>\n",
       "      <td>https://www.goodreads.com/book/show/25407018-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R for Data Science: Import, Tidy, Transform, V...</td>\n",
       "      <td>R for Data Science: Import, Tidy, Transform, V...</td>\n",
       "      <td>\\n\\nHadley Wickham (Goodreads Author), \\n\\n\\nG...</td>\n",
       "      <td>[Hadley Wickham (Goodreads Author), Garrett Gr...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>4.56</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>1,062 ratings</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>17 editions</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Buys on Amazon</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://www.goodreads.com/book/show/33399049-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning For Absolute Beginners: A Pla...</td>\n",
       "      <td>Machine Learning For Absolute Beginners: A Pla...</td>\n",
       "      <td>\\n\\nOliver Theobald\\n\\n</td>\n",
       "      <td>[Oliver Theobald]</td>\n",
       "      <td>GenresTechnologyNonfictionArtificial Intellige...</td>\n",
       "      <td>168 pages</td>\n",
       "      <td>168 pages, Kindle Edition</td>\n",
       "      <td>4.13</td>\n",
       "      <td>4.13</td>\n",
       "      <td>361 ratings</td>\n",
       "      <td>361 ratings</td>\n",
       "      <td>1 edition</td>\n",
       "      <td>Published June 21, 2017</td>\n",
       "      <td>[168 pages, Kindle Edition, Published June 21,...</td>\n",
       "      <td>To buy the newest edition of this book (2021)...</td>\n",
       "      <td>To buy the newest edition of this book (2021)...</td>\n",
       "      <td>Unlimited</td>\n",
       "      <td>[Kindle Unlimited $0.00, Kindle Unlimited $0.00]</td>\n",
       "      <td>[( To buy the newest edition of this book (202...</td>\n",
       "      <td>https://www.goodreads.com/book/show/35518108-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Data Science for Business: What You Need to Kn...   \n",
       "1  Data Smart: Using Data Science to Transform In...   \n",
       "2  Data Science from Scratch: First Principles wi...   \n",
       "3  R for Data Science: Import, Tidy, Transform, V...   \n",
       "4  Machine Learning For Absolute Beginners: A Pla...   \n",
       "\n",
       "                                              title1  \\\n",
       "0  Data Science for Business: What You Need to Kn...   \n",
       "1  Data Smart: Using Data Science to Transform In...   \n",
       "2  Data Science from Scratch: First Principles wi...   \n",
       "3  R for Data Science: Import, Tidy, Transform, V...   \n",
       "4  Machine Learning For Absolute Beginners: A Pla...   \n",
       "\n",
       "                                              author  \\\n",
       "0          \\n\\nFoster Provost, \\n\\n\\nTom Fawcett\\n\\n   \n",
       "1                            \\n\\nJohn W. Foreman\\n\\n   \n",
       "2                                  \\n\\nJoel Grus\\n\\n   \n",
       "3  \\n\\nHadley Wickham (Goodreads Author), \\n\\n\\nG...   \n",
       "4                            \\n\\nOliver Theobald\\n\\n   \n",
       "\n",
       "                                        author_names  \\\n",
       "0                      [Foster Provost, Tom Fawcett]   \n",
       "1                                  [John W. Foreman]   \n",
       "2                                        [Joel Grus]   \n",
       "3  [Hadley Wickham (Goodreads Author), Garrett Gr...   \n",
       "4                                  [Oliver Theobald]   \n",
       "\n",
       "                                              genres      num_pages  \\\n",
       "0                                          Not Found  Not Available   \n",
       "1  GenresBusinessNonfictionTechnologyProgrammingS...      409 pages   \n",
       "2  GenresProgrammingComputer ScienceTechnologyNon...      330 pages   \n",
       "3                                          Not Found  Not Available   \n",
       "4  GenresTechnologyNonfictionArtificial Intellige...      168 pages   \n",
       "\n",
       "                   paperback rating    rating1   rating_count  rating_count1  \\\n",
       "0              Not Available   4.13  Not Found  2,337 ratings      Not Found   \n",
       "1       409 pages, Paperback   4.11       4.11    988 ratings    988 ratings   \n",
       "2  330 pages, Kindle Edition   3.91       3.91  1,018 ratings  1,018 ratings   \n",
       "3              Not Available   4.56  Not Found  1,062 ratings      Not Found   \n",
       "4  168 pages, Kindle Edition   4.13       4.13    361 ratings    361 ratings   \n",
       "\n",
       "      editions     publication_info_firstedition  \\\n",
       "0  23 editions                     Not Available   \n",
       "1   5 editions  First published October 31, 2013   \n",
       "2  25 editions    First published April 14, 2015   \n",
       "3  17 editions                     Not Available   \n",
       "4    1 edition           Published June 21, 2017   \n",
       "\n",
       "                      publication_info_firstedition1  \\\n",
       "0                                      Not Available   \n",
       "1  [409 pages, Paperback, First published October...   \n",
       "2  [330 pages, Kindle Edition, First published Ap...   \n",
       "3                                      Not Available   \n",
       "4  [168 pages, Kindle Edition, Published June 21,...   \n",
       "\n",
       "                                            foreward  \\\n",
       "0                                          Not Found   \n",
       "1  Data Science gets thrown around in the press l...   \n",
       "2  \\nData science libraries, frameworks, modules,...   \n",
       "3                                          Not Found   \n",
       "4   To buy the newest edition of this book (2021)...   \n",
       "\n",
       "                                           foreward1           price  \\\n",
       "0                                          Not Found  Buys on Amazon   \n",
       "1  Data Science gets thrown around in the press l...          $27.00   \n",
       "2  \\nData science libraries, frameworks, modules,...              on   \n",
       "3                                          Not Found  Buys on Amazon   \n",
       "4   To buy the newest edition of this book (2021)...       Unlimited   \n",
       "\n",
       "                                             price1  \\\n",
       "0                                                []   \n",
       "1                    [Kindle $27.00, Kindle $27.00]   \n",
       "2                                                []   \n",
       "3                                                []   \n",
       "4  [Kindle Unlimited $0.00, Kindle Unlimited $0.00]   \n",
       "\n",
       "                                             reviews  \\\n",
       "0                                                 []   \n",
       "1  [(Data Science gets thrown around in the press...   \n",
       "2  [(\\nData science libraries, frameworks, module...   \n",
       "3                                                 []   \n",
       "4  [( To buy the newest edition of this book (202...   \n",
       "\n",
       "                                         product_url  \n",
       "0  https://www.goodreads.com/book/show/17912916-d...  \n",
       "1  https://www.goodreads.com/book/show/17682206-d...  \n",
       "2  https://www.goodreads.com/book/show/25407018-d...  \n",
       "3  https://www.goodreads.com/book/show/33399049-r...  \n",
       "4  https://www.goodreads.com/book/show/35518108-m...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define column titles\n",
    "cols = ['title', 'title1', 'author', 'author_names', 'genres', 'num_pages', 'paperback', 'rating', 'rating1', 'rating_count', 'rating_count1', \n",
    "                       'editions', 'publication_info_firstedition', 'publication_info_firstedition1', 'foreward', 'foreward1', 'price', 'price1',\n",
    "                       'reviews', 'product_url']\n",
    "\n",
    "\n",
    "#preview the scraped data as a dataframe\n",
    "df = pd.DataFrame(items, columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf79c06-3d3b-46c0-ba40-0cc0986eb33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a date_time signature to save each scraped data with\n",
    "current_dateTime = datetime.now()\n",
    "month = current_dateTime.month\n",
    "day = current_dateTime.day\n",
    "hour = current_dateTime.hour\n",
    "minute = current_dateTime.minute\n",
    "seconds = current_dateTime.second\n",
    "file_name = f'{search_query}_{month}_{day}_{hour}_{minute}'\n",
    "\n",
    "#save dataframe to csv\n",
    "df.to_csv('{0}.csv'.format(file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e014c6c-dde5-4a5e-b69a-88ee81b94718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1886 entries, 0 to 1885\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   title                           1886 non-null   object\n",
      " 1   title1                          1886 non-null   object\n",
      " 2   author                          1886 non-null   object\n",
      " 3   author_names                    1886 non-null   object\n",
      " 4   genres                          1860 non-null   object\n",
      " 5   num_pages                       1886 non-null   object\n",
      " 6   paperback                       1886 non-null   object\n",
      " 7   rating                          1886 non-null   object\n",
      " 8   rating1                         1886 non-null   object\n",
      " 9   rating_count                    1886 non-null   object\n",
      " 10  rating_count1                   1886 non-null   object\n",
      " 11  editions                        1886 non-null   object\n",
      " 12  publication_info_firstedition   1886 non-null   object\n",
      " 13  publication_info_firstedition1  1886 non-null   object\n",
      " 14  foreward                        1838 non-null   object\n",
      " 15  foreward1                       1838 non-null   object\n",
      " 16  price                           1886 non-null   object\n",
      " 17  price1                          1886 non-null   object\n",
      " 18  reviews                         1886 non-null   object\n",
      " 19  product_url                     1886 non-null   object\n",
      "dtypes: object(20)\n",
      "memory usage: 294.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Have a view of the initial scrape summary info.\n",
    "\n",
    "first_batch = pd.read_excel('data_files\\data+science_9_18_13_11_page70.xlsx') #the iteration was interrupted by network response delay\n",
    "second_batch = pd.read_excel('data_files\\data+science_9_18_15_9_page71+.xlsx') #this is the second batch\n",
    "\n",
    "initial_scrape = pd.concat([first_batch, second_batch], ignore_index=True, axis=0)\n",
    "\n",
    "initial_scrape.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a07774-72a0-49ad-a771-bf6414aa8953",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.3 OBSERVATIONS FROM SCRAPING ALL PAGES 📝\n",
    "\n",
    "1. Some books appear multiple times. May be due to multiple editions that are not propoerly differentiated.\n",
    "2. The random nature of the reposnse from each call on the shelves/pages or book url makes it necessary to individually scrape some properties with huge missingness. Only _Book Title_, _Author Name(s)_ & _product_URL_ columns had zero missing/unreported data, while other attributes had partial responses for expected data.\n",
    "3. Some books with unknown authors and random data.\n",
    "4. Adding few sleep calls within the lines improved the response from each page call.\n",
    "5. It takes an average of __225 seconds__ (excluding sleep duraton) to scrape each page for all the data collected as in the previous code line.\n",
    "6. There is the need to run another re-iteration for these columns, to fill the initial scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6296d2-680b-436f-ae70-8b4fa9fd32ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.0 IMPROVING THE INITIAL SCRAPE 🛠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc12ed-40dd-48c8-80f4-0db4cbacf8de",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "#### 4.1 RERUN REQUESTS TO RESCRAPE INCOMPLETE COLUMNS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4e00b-d658-4a6a-86f9-edeee5afdf6d",
   "metadata": {},
   "source": [
    "To improve the initial scrape the dataframe will be filtered, choosing one column at a time (preferrably), for rows with missing data. The rows will be updated by making another request using the unique _product_URL_. With the new row appended to the dataframe, the old row with missing feature will be dropped.\n",
    "\n",
    "The function in the next lines accomplishes this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fcf7fd6-02fb-498d-b3d3-f90bf264e3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter for rows with incomplete data in the chosen column\n",
    "df_url_all = pd.read_excel('auto_updated.xlsx')\n",
    "df_url_all = df_url_all[df_url_all['five_star']=='Not Found']# | (df_url['reviews']=='[]') | (df_url['foreward']=='Not Found') |(df_url['rating']=='')]\n",
    "df_url_all = df_url_all['product_url']\n",
    "len(df_url_all)#.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14227f52-4663-4a25-a555-a2aa96b20deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-run for incompletely scraped columns\n",
    "retry_result = []\n",
    "for i in tqdm(df_url_all):\n",
    "    response2 = requests.get(i, headers=headers)\n",
    "    sleep(4)\n",
    "    soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "    \n",
    "    # try:\n",
    "    #     rating1 = soup2.find('div' , {'class':'RatingStatistics__rating'}).text\n",
    "    # except AttributeError:\n",
    "    #     rating1 = 'Not Found'\n",
    "    \n",
    "    # try:\n",
    "    #     foreward = soup2.find('span' , {'class': 'Formatted', }).text\n",
    "    # except AttributeError:\n",
    "    #     foreward = 'Not Found'\n",
    "    \n",
    "#     first30_reviews=soup2.find_all('div' , {'class':'TruncatedContent__text'})\n",
    "#     reviews = [(rev.text, \"\\n\") for rev in first30_reviews]\n",
    "    \n",
    "    # try:\n",
    "    #     publication_info_firstedition1 = [i.text for i in soup2.find('div' , {'class':'FeaturedDetails'})]\n",
    "    # except (AttributeError, TypeError):\n",
    "    #     publication_info_firstedition1 = 'Not Found'\n",
    "    \n",
    "    try:\n",
    "        num_pages=soup2.find('p' , {'data-testid':'pagesFormat'}).text.split(', ')[0]\n",
    "    except AttributeError:\n",
    "        num_pages = 'Not Found'\n",
    "    \n",
    "    # try:\n",
    "    #     price = soup2.find_all('div' , {'class':'BookActions__button'})[-1].text.split(' ')[1]\n",
    "    # except IndexError:\n",
    "    #     price = 'Amazon'\n",
    "\n",
    "    # try:\n",
    "    #     price1 = [i.text for i in soup2.find_all('span' , {'class':'Button__labelItem'}) if '$' in i.text]\n",
    "    # except AttributeError:\n",
    "    #     price1 = 'Amazon'\n",
    "    \n",
    "#     try:\n",
    "#         rating1 = soup2.find('div' , {'class':'RatingStatistics__rating'}).text\n",
    "#     except AttributeError:\n",
    "#         rating1 = 'Not Found'\n",
    "    \n",
    "#     try:\n",
    "#         genres = soup2.find('ul' , {'class': 'CollapsableList'}).text\n",
    "#     except AttributeError:\n",
    "#         genres = 'Not Found'\n",
    "\n",
    "\n",
    "    #data = [price, price1, rating1, num_pages, genres, foreward, publication_info_firstedition1, reviews, i]\n",
    "    #retry_result.append([five_stars, four_stars,three_stars,two_stars,one_stars,price,i])\n",
    "    retry_result.append([num_pages,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2a512-cdd2-4be6-a082-20549b8a49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview result\n",
    "pd.DataFrame(retry_result)#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64db748-6211-4174-b461-4454a6276822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_changes(new_list, old_file_path, col_name, marker):\n",
    "    \"\"\"\n",
    "    This function is used to update the original file with the new values\n",
    "    from the re-scrapes process done for each column...\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    new_list : List object\n",
    "        The new list to be used to update specified columns on desired file.\n",
    "    old_file_path : str\n",
    "        The file path to the file whose column/columns needs to be updated.\n",
    "    col_name : str\n",
    "        The specific column to be updated at the instance.\n",
    "    marker : str\n",
    "        The content in the column which indicates rows/cells to be updated.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        The modified dataframe.\n",
    "    \"\"\"\n",
    "    #make the new values into a dataframe and rename the columns appropriately\n",
    "    new_col_names = [f'new_{col_name}', 'product_url']\n",
    "    new_val_df = pd.DataFrame(new_list, columns=new_col_names)\n",
    "    \n",
    "    #read in the file to be updated/edited as 'tbe'\n",
    "    try:\n",
    "        df_tbe = pd.read_excel(old_file_path)\n",
    "    except (ValueError,TypeError):\n",
    "        df_tbe = pd.read_csv(old_file_path, encoding = 'ISO-8859-1')\n",
    "    \n",
    "    #slice out the rows that require update from the specified column.\n",
    "    df_be = df_tbe[df_tbe[col_name]==marker]\n",
    "    \n",
    "    #convert the new values to be inserted into a list\n",
    "    updated_column_vals = new_val_df.copy()[f'new_{col_name}'].to_list()\n",
    "    #make the changes on the sliced out dataframe\n",
    "    df_be[col_name] = updated_column_vals\n",
    "    \n",
    "    #concatenate the slice and the main dataframe which now contains duplicates of rows that have been updated\n",
    "    both = pd.concat([df_tbe, df_be], axis=0)\n",
    "    \n",
    "    #drop the duplicates, keeping the newly appended row\n",
    "    cleaned = both.drop_duplicates(subset='product_url',\n",
    "                     keep='last')\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85f03a-b50d-4be0-ba44-257598f40579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare variables for the function, or update per column being worked on\n",
    "new_list = retry_result\n",
    "old_file_path = 'auto_updated.xlsx'\n",
    "col_name = 'num_pages'\n",
    "marker = 'Not Found'\n",
    "\n",
    "#execute update and check if some/all rows of the interested columns have been updated\n",
    "df_updated = _make_changes(new_list=new_list, old_file_path=old_file_path, col_name=col_name, marker=marker)\n",
    "\n",
    "diff = len(df_url_all) - len(df_updated[df_updated[col_name]==marker])\n",
    "remnant = len(df_updated[df_updated[col_name]==marker])\n",
    "\n",
    "print(f'{diff} rows have been updated, {remnant} more needs update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c8d08-21c2-42c5-9c79-bc79233f6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the newly updated file to csv on specified directory\n",
    "df_updated.to_excel('auto_updated.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9403e2-cd6b-4234-b456-91a8de22970d",
   "metadata": {},
   "source": [
    "[Return to section 4.1 here and iterate until the field/column is completely filled](#4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce27195-5c2d-488b-ae8f-e01bc632f918",
   "metadata": {},
   "source": [
    "### 5.0 SCRAPING REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610586e-136e-4785-8d9d-21375e5fe416",
   "metadata": {},
   "source": [
    "It is necessary to scrape reviews separately from other data due to the structure required for the analysis to be carried out; each review is independent regardless of the book being reviewed. The interest here is the review text and the _star_ rating that comes with each.\n",
    "\n",
    "The book URLs from the previous scraping step will be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f02403-b206-4cac-b3ad-4c40c297b9cb",
   "metadata": {},
   "source": [
    "***\n",
    "First the absolute book address is extracted from the full URL.\n",
    "\n",
    "For instance, _URL_1_ below is the default address for the book called '100 Puzzles and Case Studies To Crack Data Science Interview' while _URL_2_ is the URL for the reviews page of the same book.\n",
    "\n",
    "1. https://www.goodreads.com/book/show/35296800-100-puzzles-and-case-studies-to-crack-data-science-interview\n",
    "2. https://www.goodreads.com/book/show/35296800/reviews\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20c817-16e2-4184-b052-b1fc8f54e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data from the previous scrape, split URL for each book\n",
    "#and obain these as a series to be used for iteration\n",
    "url_for_revs = pd.read_excel('Goodread_InitialFullScrape_deldup.xlsx')\n",
    "\n",
    "#filter for books/rows with at least one rating\n",
    "url_for_revs = url_for_revs[url_for_revs['rating_count']!='0 ratings']\n",
    "\n",
    "#engineer the absolute reviews page URL for each book\n",
    "url_for_revs['abs_url'] = url_for_revs['product_url'].str.split('-',expand=True)[0] + '/reviews'\n",
    "url_for_revs = url_for_revs['abs_url']\n",
    "\n",
    "url_for_revs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba456b66-11fa-44cc-8a53-fe17674fc9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ = []\n",
    "for i in tqdm(url_for_revs[69:-1]):\n",
    "    \n",
    "    response3 = requests.get(i, headers=headers)\n",
    "    sleep(2)\n",
    "    soup3 = BeautifulSoup(response3.text, 'lxml')\n",
    "\n",
    "    rev_sec = soup3.find_all('article', {'class':'ReviewCard'})\n",
    "    revs = [d.text for d in soup3.find_all('span', {'class':'Formatted'})]\n",
    "    \n",
    "    #There are usually 30 reviews available at once on each book review page,\n",
    "    #if total reveiws up to or more than 30\n",
    "    \n",
    "    \n",
    "    for r in range(len(revs)):\n",
    "        #book urls not separated by '-' separator will return an Index error\n",
    "        try:\n",
    "            user_review = revs[r]\n",
    "            start = str(rev_sec[r]).find('aria-label=\"Rating ')\n",
    "            user_rating = str(rev_sec[r])[start+18:start+29]\n",
    "            reviews_.append([user_review, user_rating,i])\n",
    "        except IndexError:\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3cd3c-ae0d-41e6-a94d-da31c28f4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print lenght of reviews\n",
    "len(reviews_)\n",
    "\n",
    "#save to csv\n",
    "pd.DataFrame(reviews_).to_csv('reviews2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6740b-eec6-4daf-83ac-f8c1c23b6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = requests.get(url_for_revs.to_list()[12], headers=headers)\n",
    "soup3 = BeautifulSoup(response3.text, 'lxml')\n",
    "\n",
    "rev_sec = soup3.find_all('article', {'class':'ReviewCard'})#[1].get_text()#find_all('span', {'class':'Formatted'})\n",
    "revs = [i.text for i in rev_sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a4c65-1e23-4010-beeb-7970424622fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.read_csv('Goodread_InitialFullScrape_180923.csv', encoding = 'ISO-8859-1')\n",
    "dfb.query('rating'=='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "337870fc-e25d-4793-80c1-ac3e19a809f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_url_all = pd.read_excel('auto_updated.xlsx')\n",
    "# #df_url = df_url[(df_url['price'].str.contains('Amazon')) | (df_url['reviews']=='[]') | (df_url['foreward']=='Not Found') |(df_url['rating']=='')]\n",
    "df_url_all = df_url_all[df_url_all['five_star']=='Not Found']\n",
    "df_url_all = df_url_all['product_url']\n",
    "len(df_url_all)#.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "302464cb-03ac-407f-bd1b-e3139ef29990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.16s/it]\n"
     ]
    }
   ],
   "source": [
    "stars_list = []\n",
    "for i in tqdm(df_url_all):\n",
    "    response2 = requests.get(i, headers=headers)\n",
    "    sleep(4)\n",
    "    soup2 = BeautifulSoup(response2.text, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        five_stars=soup2.find('div' , {'class':'RatingsHistogram__labelTotal', 'data-testid':'labelTotal-5'}).text\n",
    "        four_stars=soup2.find('div' , {'class':'RatingsHistogram__labelTotal', 'data-testid':'labelTotal-4'}).text\n",
    "        three_stars=soup2.find('div' , {'class':'RatingsHistogram__labelTotal', 'data-testid':'labelTotal-3'}).text\n",
    "        two_stars=soup2.find('div' , {'class':'RatingsHistogram__labelTotal', 'data-testid':'labelTotal-2'}).text\n",
    "        one_stars=soup2.find('div' , {'class':'RatingsHistogram__labelTotal', 'data-testid':'labelTotal-1'}).text\n",
    "    except AttributeError:\n",
    "        five_stars,four_stars,three_stars,two_stars,one_stars = 'Not Found','Not Found','Not Found','Not Found','Not Found'\n",
    "    \n",
    "    stars = [i,five_stars,four_stars,three_stars,two_stars,one_stars]\n",
    "    stars_list.append(stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7155337-f00d-4c97-8952-4ba24c10009d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.goodreads.com/book/show/58437650-t...</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "      <td>Not Found</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1          2  \\\n",
       "0  https://www.goodreads.com/book/show/58437650-t...  Not Found  Not Found   \n",
       "\n",
       "           3          4          5  \n",
       "0  Not Found  Not Found  Not Found  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#res = pd.DataFrame({'price': prices_list})\n",
    "res = pd.DataFrame(stars_list)\n",
    "res.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8820c-c353-4834-bf99-0c07829fbf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'retry_stars8'\n",
    "res.to_csv('{0}.csv'.format(file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee60841-f4b7-4b4f-b233-9b57f5050bab",
   "metadata": {},
   "source": [
    "### REFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cedaa7-e9ad-4a26-874f-428abab6d541",
   "metadata": {},
   "source": [
    "* [Scraping Dynamic HTML content in a Flex Container](https://discourse.mcneel.com/t/extract-specific-html-with-a-flex-container-using-ironpython-in-gh/141082/5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
